{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8274f4ad-2888-4958-a0fe-adaecdea2ece",
   "metadata": {},
   "source": [
    "# Document-Based Question Answering using RAG\n",
    "\n",
    "This notebook demonstrates a basic Retrieval Augmented Generation (RAG) pipeline.\n",
    "The system answers user questions by retrieving relevant information from documents\n",
    "and passing that context to a Large Language Model (LLM).\n",
    "\n",
    "**Tech Stack:** Python, LangChain, FAISS, Hugging Face Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99442dc-9ec9-427f-9c1f-5fa1b6c6aea2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîπ Step 1: Install Required Libraries\n",
    "\n",
    "```python\n",
    "!pip install langchain langchain-community langchain-huggingface faiss-cpu pypdf sentence-transformers\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f417f1-1aac-47a8-bd26-3b0d00a25a38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\public\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\public\\anaconda3\\lib\\site-packages (0.3.31)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.1-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\public\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (0.3.78)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (0.4.21)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.34.4)\n",
      "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.22.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\public\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\public\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\public\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\public\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\public\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\public\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\public\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\public\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\public\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\public\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\public\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\public\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\public\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\public\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\public\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\public\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\public\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\public\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\public\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Using cached langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading faiss_cpu-1.13.1-cp312-cp312-win_amd64.whl (18.8 MB)\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/18.8 MB 10.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.7/18.8 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.2/18.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.0/18.8 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.8/18.8 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 7.9/18.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.4/18.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 8.9/18.8 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 9.7/18.8 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 10.5/18.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.0/18.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.8/18.8 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.3/18.8 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 13.1/18.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 13.6/18.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 14.2/18.8 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.7/18.8 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.7/18.8 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 15.5/18.8 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.0/18.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.5/18.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.3/18.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.8/18.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.8/18.8 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading pypdf-6.4.2-py3-none-any.whl (328 kB)\n",
      "Installing collected packages: pypdf, faiss-cpu, langchain-huggingface\n",
      "Successfully installed faiss-cpu-1.13.1 langchain-huggingface-0.3.1 pypdf-6.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-huggingface faiss-cpu pypdf sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a7dc1-bf24-4866-b5ce-74d85b1b721d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîπ Step 2: Import Required Libraries (Code Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7be40be-324e-4192-b22f-e4c45ad7b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd505eca-9772-454b-b353-adf2377aaeaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîπ Step 3: Load the Document\n",
    "\n",
    "We load a PDF document which will act as our knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac80bb1c-75d6-4b10-924c-074b94361add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 20\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = r\"C:\\Users\\Chethan Vakiti\\Downloads\\Machine Learning Notes.pdf\"  \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Number of pages loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25583bd-ad1d-4a83-a103-06df6f513707",
   "metadata": {},
   "source": [
    "\n",
    "üëâ Interview line:\n",
    "\n",
    "> ‚ÄúI load documents using LangChain loaders to convert them into text format.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 4: Split Document into Chunks \n",
    "Chunking helps embeddings capture semantic meaning effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c90127ed-946b-400c-a1eb-244c5fda7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 46\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap =50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3b387-92e9-4dfc-832a-68bcd9415eff",
   "metadata": {},
   "source": [
    "üëâ Interview line:\n",
    "\n",
    "> ‚ÄúChunking prevents loss of context and improves retrieval accuracy.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 5: Create Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db69b92c-6476-411a-9b32-e4f4fe835b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Public\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49a74d-995a-47e0-9451-949a4255ebdf",
   "metadata": {},
   "source": [
    "üëâ Interview line:\n",
    "\n",
    "> ‚ÄúEmbeddings allow similarity-based retrieval instead of keyword matching.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step 6: Store Embeddings in Vector Database \n",
    "FAISS helps us perform fast similarity search on embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25f753b6-9480-418a-875b-c005bf76ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24a4d8-5c6e-4625-8ee0-0d8f3ead58b5",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Step 7: Create Retriever\n",
    "\n",
    "The retriever fetches the most relevant chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18606c71-f0ce-49d0-a3d0-6eb80542ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a41b5b-38aa-4e8b-a5c5-744ad4b80c5e",
   "metadata": {},
   "source": [
    "## üîπ Step 8: Test Semantic Retrieval (Core RAG Component)\n",
    "\n",
    "Before using an LLM, we test whether relevant content is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb4912eb-1d2d-4564-aa3f-ed90102c7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Chunk 1 ---\n",
      "SUPERVISED LEARNING:\n",
      "What is it?\n",
      "In Supervised Learning, the model is trained on a labeled dataset,\n",
      "meaning every training example has an input (X) and a\n",
      "corresponding correct output (Y). The goal is for the model to learn\n",
      "the relationship between inputs and outputs so it can predict outputs\n",
      "for new\n",
      "\n",
      "--- Retrieved Chunk 2 ---\n",
      "Common Classification Algorithms:\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "4/17/25, 11:28 AM Editing Machine Learning ‚Äì Medium\n",
      "https://medium.com/p/2d3efa24e2a6/edit 7/20\n",
      "\n",
      "--- Retrieved Chunk 3 ---\n",
      "üîπHybrid Systems\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "‚Ä¢\n",
      "4/17/25, 11:28 AM Editing Machine Learning ‚Äì Medium\n",
      "https://medium.com/p/2d3efa24e2a6/edit 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chethan Vakiti\\AppData\\Local\\Temp\\ipykernel_9032\\1466237282.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is this document about?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Retrieved Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:300])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850a79f-71c0-4784-82fd-2020d9dbe766",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "This project focuses on implementing and validating the **retrieval component** of a\n",
    "Retrieval Augmented Generation (RAG) system.\n",
    "\n",
    "The retrieved document context can be passed to any Large Language Model (LLM)\n",
    "such as Gemini or OpenAI for answer generation. This approach improves accuracy\n",
    "and reduces hallucinations compared to direct LLM usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb0f8b-0b37-447c-ae02-55f5ef0f8878",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ Step 9: Final Explanation \n",
    "\n",
    "## Final Explanation\n",
    "\n",
    "1. Load and split documents into chunks  \n",
    "2. Convert chunks into embeddings  \n",
    "3. Store embeddings in a vector database  \n",
    "4. Retrieve relevant chunks for a user query  \n",
    "5. Pass retrieved context to an LLM for answer generation  \n",
    "\n",
    "This approach improves accuracy and reliability compared to direct LLM usage.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Overview of this project\n",
    "\n",
    "\n",
    "> ‚ÄúI built a document-based question answering system using RAG. I load and split documents, generate embeddings, store them in a vector database, and retrieve relevant chunks based on user queries. These chunks are then provided to the LLM to generate answers. This reduces hallucinations and ensures answers are grounded in document context.‚Äù\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f6fdc-8ebd-4430-ba6e-3b737ceacb95",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ÄúI implemented a document-based question answering system using RAG. I load PDFs using LangChain, split them into chunks, generate embeddings using Hugging Face models, and store them in a FAISS vector database. When a user asks a question, I retrieve the most relevant chunks using semantic search and pass that context to an LLM for answer generation. This approach reduces hallucinations compared to direct LLM usage.‚Äù\n",
    "\n",
    "\n",
    "‚ÄúI implemented and validated the retrieval component of a RAG system. The retrieved document context can be passed to any LLM such as Gemini or OpenAI for answer generation. I focused on retrieval accuracy since that is the core of RAG.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5295656-84cd-424a-8bda-53dd74906261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
